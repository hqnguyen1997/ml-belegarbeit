{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bericht und Code zum Beleg Maschinelles Lernen\n",
    "\n",
    "\n",
    "Name:    \n",
    "Matrikelnummer:\n",
    "\n",
    "Name:   \n",
    "Matrikelnummer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gliederung\n",
    "\n",
    "* [Code](#Code)\n",
    "* [Bericht](#Bericht)\n",
    " * [Bewertung der Daten](#Bewertung-der-Daten)\n",
    " * ..\n",
    " * [Evaluation auf der Testmenge](Evaluation-auf-der-Testmenge)\n",
    " * [Literaturverzeichnis](#Literaturverzeichnis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def fit(self,*_):\n",
    "        return self\n",
    "    def transform(self,x,y=None,**fit_params):\n",
    "        tokenized=df.apply(lambda row: nltk.word_tokenize(row['title']+\" \"+row['text'], language='english'), axis=1)\n",
    "        stop_words = set(stopwords.words('english')) \n",
    "        return x.apply(lambda row: [w for w in row['title'] if not w in stop_words], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Die Datei sollte in dem selben Verzeichnis liegen, wie das notebook\n",
    "# bitte den Pfad NICHT ändern, sodass es bei mir gleich durchläuft!\n",
    "df = pd.read_csv('data_beleg.csv', encoding='utf-8')\n",
    "\n",
    "# The column \"label\" is the target variable y which should be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3250</td>\n",
       "      <td>Strawberry Pistachio Crumble A Spicy Perspective</td>\n",
       "      <td>When you re out in the berry patch drinking in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4311</td>\n",
       "      <td>BBC News Study links Parkinson s disease to in...</td>\n",
       "      <td>13 November 2011 Last updated at 23 57 ET By N...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6544</td>\n",
       "      <td>Want to know the effect of a nuclear bomb on y...</td>\n",
       "      <td>By Damien Gayle UPDATED 13 12 EST 16 February ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5328</td>\n",
       "      <td>Japanese Nail Art by NeverTooMuchGlitter by Ne...</td>\n",
       "      <td>Check out my featured work in the August Septe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1402</td>\n",
       "      <td>How the New 5 Euro Bill is Made</td>\n",
       "      <td>A new 5 euro note will be circulating starting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>2708</td>\n",
       "      <td>Halloween Dessert Recipe Chocolate Abyss hallo...</td>\n",
       "      <td>This delicious dish was created for the M M s ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4647</th>\n",
       "      <td>6324</td>\n",
       "      <td>eat me delicious Cauliflower Gratin</td>\n",
       "      <td>Looking at this picture I m not sure if you ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4648</th>\n",
       "      <td>2777</td>\n",
       "      <td>Unusual Medical Uses For Duct Tape</td>\n",
       "      <td>Duct Tape is possibly the most incredible inve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4649</th>\n",
       "      <td>1508</td>\n",
       "      <td>Reporter Flips Out from That Happened reporter...</td>\n",
       "      <td>reporter flips out, news blooper, bug, south,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4650</th>\n",
       "      <td>5336</td>\n",
       "      <td>Top 10 Web 2 0 Startups Taking Over the World</td>\n",
       "      <td>Written By Drew August 22nd 2011 If one looks ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4651 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "0           3250  Strawberry Pistachio Crumble A Spicy Perspective    \n",
       "1           4311  BBC News Study links Parkinson s disease to in...   \n",
       "2           6544  Want to know the effect of a nuclear bomb on y...   \n",
       "3           5328  Japanese Nail Art by NeverTooMuchGlitter by Ne...   \n",
       "4           1402                   How the New 5 Euro Bill is Made    \n",
       "...          ...                                                ...   \n",
       "4646        2708  Halloween Dessert Recipe Chocolate Abyss hallo...   \n",
       "4647        6324               eat me delicious Cauliflower Gratin    \n",
       "4648        2777                Unusual Medical Uses For Duct Tape    \n",
       "4649        1508  Reporter Flips Out from That Happened reporter...   \n",
       "4650        5336     Top 10 Web 2 0 Startups Taking Over the World    \n",
       "\n",
       "                                                   text  label  \n",
       "0     When you re out in the berry patch drinking in...      1  \n",
       "1     13 November 2011 Last updated at 23 57 ET By N...      1  \n",
       "2     By Damien Gayle UPDATED 13 12 EST 16 February ...      1  \n",
       "3     Check out my featured work in the August Septe...      0  \n",
       "4     A new 5 euro note will be circulating starting...      1  \n",
       "...                                                 ...    ...  \n",
       "4646  This delicious dish was created for the M M s ...      0  \n",
       "4647  Looking at this picture I m not sure if you ca...      1  \n",
       "4648  Duct Tape is possibly the most incredible inve...      1  \n",
       "4649   reporter flips out, news blooper, bug, south,...      0  \n",
       "4650  Written By Drew August 22nd 2011 If one looks ...      0  \n",
       "\n",
       "[4651 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bericht\n",
    "\n",
    "## Bewertung der Daten\n",
    "\n",
    "Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ist natürlich auch Code erlaubt. Es sollte aber übersichtlich bleiben!\n",
    "\n",
    "Graphiken können auch mit Code erzeugt werden. Helper Funktionen eventuell in Code-Bereich auslagern. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Text Feature Extraction\n",
    "Tokenizer -> Text Vektorisierung mit Bag of Words + TF-IDF (Erklärung und Exkurs mit Skicit learn)\n",
    "* Bag of Words (BoW): BoW ist eine einfache Repräsentation, die in \"Natural Language Processing\" und \"Information Retrieval verwendet werden. In diesem Model wird Text als ein \"Bag\" von Worten repräsentiert, unabhängigt von Grammatik oder sogar Reihenfolge, aber die Anzahl der Erscheinungen (Häufigkeit) eines Wortes wird behalten. z.B: \"How the New 5 Euro Bill is Made, A new 5 euro note will be circulating starting\", \" wird so repräsentiert: \n",
    "    \n",
    "    {\"how\":1, \"the\":1, \"new\": 2, \"5\": 2, \"euro\": 2, \"bill\": 1, \"is\": 1, \"made\": 1, \"a\": 1, \"note\": 1, \"will\": 1, \"be\": 1, \"circulating\": 1, \"starting\": 1}\n",
    "    \n",
    "    \n",
    "* Das BoW Model hat auch einen Nachteil beim Text Klassifikator. Nur die Häufigkeit der Worten ist nicht genug für Klassifikation. Die übliche Worte wie (Deutsch) \"der\", \"die\", \"das\" spielen beim Traning des Klassifikators nah zu keine Rolle und können sogar das Ergebnis des Tranings negativ beeinflussen. Dieser Nachteil lässt sich in der Praxis meist mit Tf-idf Algorithmus ausgleichen, indem Worte anhand deren Häufigkeit in einem Dokument und der Häufigkeit in einem Satz von Dokumenten gewichtet werden. Wenn ein Wort häufig in einem Dokument erscheint, jedoch nicht in anderen Dokumenten, hat das Wort eine größere Wichtung. Wenn ein Wort überall häufig erscheint, ist das möglicherweise unwichtig und wird runter bewerten. Ziel des Algorithmus ist, die häufig vorkommende Worte wie \"der\", \"die\", \"das\" zu normalisieren, weil die nicht wichtig für das Training sind.\n",
    "\n",
    "\n",
    "* Die Kombination von BoW und Tf-idf lässt sich sehr einfach mit Scikit-learn implementieren. Die Klasse \"CountVectorizer\" und \"TfidfTransformer\" vom Paket \"sklearn.feature_extraction.text\" können in der Pipeline verwendet werden. \n",
    "\n",
    "    Nachfolgend ein Beispiel, wie die Pipeline aussehen könnte:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                          ('tfidf', TfidfTransformer())\n",
    "                         #weitere Elemente für Pipeline\n",
    "                        ])\n",
    "\n",
    "```\n",
    "\n",
    "### Exkurs: Einsatz eines Stemmer\n",
    "Beispiel: https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "### Feature Selection mittels Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Klassifikator \n",
    "### Tunning Parameter für jedes Model Grid Search / Random Search\n",
    "\n",
    "Für jedes Model, das man für das Training verwenden würde, sind die Parameter ein unverzichbarer Teil. z.B beim Random Forest sind die Parameter beispielsweise: n_estimators, max_features, max_depth, min_samples_split.  Unterschiedliche Parameter Kombinationen beeinflussen die Genauigkeit des Models und der Vorhersage. Abhängigt von konkreten Problem und Anzahl der vorhandenen Daten sind die passende Parameter unterschiedlich. Es ist notwendig, verschiedene Parameter auszuprobieren, um die optimalen Parameter zu finden.\n",
    "\n",
    "In der Praxis werden hauptsächlich zwei Methode verwendet, um Hyperparameter zu finden. Die sind \"Grid Search\" und \"Random Search\". Nachfolgend werden die Methode beschrieben.\n",
    "\n",
    "* Grid Search: Grid Search probiert alle Kombination von Parametern aus und berechnet die Genauigkeit des Models. Der große Vorteil dieser Methode ist, dass keine Kombination von Parametern verpasst wird, und die optimalste Parameter werden korrekt ermittelt. Demgegenüber muss man allerdings damit rechnen, dass die Versuche so lange dauern könnten, wenn es zahlreiche Parameter geben würde. Es könnte Stunde, Tage und sogar Monaten dauern.\n",
    "* Random Search: Wie der Name der Methode besagt, bei Random Search werden nur zufällige Kombinationen ausprobiert. Das heiß also, dass nicht alle Kombinationen für die Ermittlung der Genauigkeit verwendet werden und die optimalste Hyperparametern könnten verpasst werden. Im Vergleich zu Grid Search ist Random Search schneller, aber wegen dessen könnter die optimalste Kombination der Hyperparametern nicht korrekt ermittelt werden. Random Search wird empfohlen, wenn der Search Raum mehr als drei Dimensionen hat. Random Search wäre zeitlich effizienter, wenn es zu viele Parameter gibt.\n",
    "\n",
    "Beider oben genannte Methode lassen sich mit scikit-learn implementieren. Nachfolgend werden Random Search verwendet, um die besten Hyperparameter für jedes Model zu finden.\n",
    "\n",
    "### Logistic Regression Model\n",
    "### Naive Bayes Model\n",
    "### Linear Support Vector Machine ???\n",
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet\n"
     ]
    }
   ],
   "source": [
    "print (\"Lorem ipsum dolor sit amet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventuell auch Formeln zur Erläuterung,\n",
    "\n",
    "$$\n",
    "\\text{BLU} = \\int_0^1 f(x) dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unterpunkt 1\n",
    "\n",
    "At vero eos et accusam et justo duo dolores et ea rebum. Stet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesModel:\n",
    "    def __init__(self):\n",
    "        self.base_clf = self._init_pipeline()\n",
    "        self.stemmer_clf = self._init_snowballStemmer_pipeline()\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_pipeline():\n",
    "        pipe_line = Pipeline([\n",
    "            (\"transformer\", Tokenizer()),#sử dụng pyvi tiến hành word segmentation\n",
    "            (\"vect\", CountVectorizer()),#bag-of-words\n",
    "            (\"tfidf\", TfidfTransformer()),#tf-idf\n",
    "            (\"clf\", MultinomialNB())#model naive bayes\n",
    "        ])\n",
    "        return pipe_line\n",
    "    \n",
    "    @staticmethod\n",
    "    def _init_snowballStemmer_pipeline():\n",
    "        pipe_line = Pipeline([\n",
    "            (\"tokenizer\", Tokenizer()),#sử dụng pyvi tiến hành word segmentation\n",
    "            (\"vect\", CountVectorizer()),#bag-of-words\n",
    "            (\"SnowballStemmer\",SnowballStemmer(\"english\"))\n",
    "            (\"tfidf\", TfidfTransformer()),#tf-idf\n",
    "            (\"clf\", MultinomialNB())#model naive bayes\n",
    "        ])\n",
    "\n",
    "        return pipe_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-8af4961493d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnBm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNaiveBayesModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-54-cce1b05e6b0d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstemmer_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_snowballStemmer_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-cce1b05e6b0d>\u001b[0m in \u001b[0;36m_init_snowballStemmer_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m\"vect\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;31m#bag-of-words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m\"SnowballStemmer\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;33m(\u001b[0m\u001b[1;34m\"tfidf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;31m#tf-idf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[1;34m\"clf\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#model naive bayes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         ])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "nBm=NaiveBayesModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation auf der Testmenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Da Sie das Test-Set nicht haben, Führen Sie dies mit den ursprünglichen Daten durch:\n",
    "df_test = pd.read_csv('data_beleg.csv', encoding='utf-8')\n",
    "# Wird von mir ersetzt durch\n",
    "# df_test = pd.read_csv('data_test.csv', encoding='utf-8')\n",
    "# und sollte dann auch durchlaufen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true = ... # Variable für die Ground Truth des Test-Datensatzes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wenden Sie hier ihre Vorverarbeitung/Klassifikation ihres entgültigen Modells \n",
    "# an. \n",
    "\n",
    "# In dieser Variable sollte die Vorhersagewahrscheinlichkeit für die positive Klasse\n",
    "# p(y=1 | x) gespeichte sein.\n",
    "y_test_prob_class_1 = ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the ROC-AUC evaluation metric for the test data set:\n",
    "sklearn.metrics.roc_auc_score(y_test_true, y_test_prob_class_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Literaturverzeichnis\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
